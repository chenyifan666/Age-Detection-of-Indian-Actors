{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Detection of Indian Actors - version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2.0 adds to the previous version the following features/components/expts. :\n",
    " * Autoencoders\n",
    " * Advanced Activation Layers (LeakyReLU, PReLU, ELU)\n",
    " * Neural network architecture and hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from PIL import Image\n",
    "import cv2 #OpenCV library\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/paperspace/Desktop/AV - Age Detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_HOME_DIR = os.getcwd()\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the images and exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>377.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17814.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21283.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16496.jpg</td>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4487.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   Class\n",
       "0    377.jpg  MIDDLE\n",
       "1  17814.jpg   YOUNG\n",
       "2  21283.jpg  MIDDLE\n",
       "3  16496.jpg   YOUNG\n",
       "4   4487.jpg  MIDDLE"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19906</td>\n",
       "      <td>19906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>25628.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID   Class\n",
       "count       19906   19906\n",
       "unique      19906       3\n",
       "top     25628.jpg  MIDDLE\n",
       "freq            1   10804"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIDDLE    10804\n",
       "YOUNG      6706\n",
       "OLD        2396\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setting path for images\n",
    "TRAIN_PATH = DATA_HOME_DIR + '/Train/'\n",
    "TEST_PATH = DATA_HOME_DIR + '/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to read images as arrays\n",
    "def read_image(img_path, mode = 'color', resize = False, size = 32):\n",
    "    '''\n",
    "    Default mode is : color(BGR) --> color(RGB)\n",
    "    Other modes allowed are : 'grayscale' and 'include_opacity'\n",
    "    '''\n",
    "    if mode == 'grayscale':\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    elif mode == 'include_opacity':\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    else:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if resize == True:\n",
    "        img = cv2.resize(img, (size, size))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19906/19906 [00:07<00:00, 2523.91it/s]\n",
      "100%|██████████| 6636/6636 [00:02<00:00, 2530.53it/s]\n"
     ]
    }
   ],
   "source": [
    "## Storing all images as list of arrays\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for img in tqdm(train['ID'].values):\n",
    "    train_data.append(read_image(TRAIN_PATH + '{}'.format(img), resize = True, size = 32))\n",
    "    \n",
    "for img in tqdm(test['ID'].values):\n",
    "    test_data.append(read_image(TEST_PATH + '{}'.format(img), resize = True, size = 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Checking dimensions and aspect ratio of original images (without resizing)\n",
    "rows = [x.shape[0] for x in (train_data+test_data)]\n",
    "cols = [x.shape[1] for x in (train_data+test_data)]\n",
    "channels = [x.shape[2] for x in (train_data+test_data)]\n",
    "aspect_ratio = [x.shape[0]/x.shape[1] for x in (train_data+test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. and Max. rows = 11 and 720 respectively \n",
      "Min. and Max. cols = 8 and 800 respectively \n",
      "Min. and Max. channels = 3 and 3 respectively \n",
      "Min. and Max. aspect ratio = 0.42857142857142855 and 4.230769230769231 respectively \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Checking range of the dimesnions and aspect ratio\n",
    "print(\"Min. and Max. rows = {} and {} respectively \\nMin. and Max. cols = {} and {} respectively \\nMin. and Max. channels = {} and {} respectively \\nMin. and Max. aspect ratio = {} and {} respectively \\n\"\n",
    "      .format(min(rows), max(rows), min(cols), max(cols), min(channels), max(channels), min(aspect_ratio), max(aspect_ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f323420d210>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAEICAYAAAC01Po2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f323420d910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Displaying random images without resizing\n",
    "#i = np.random.choice(np.arange(len(train_data)))\n",
    "\n",
    "plt.title('{} - {} ; shape : {}'.format(train['ID'].values[i], y_train[i], train_data[i].shape))\n",
    "plt.imshow(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Scaling the images\n",
    "X_train = np.array(train_data, np.float32) / 255.\n",
    "X_test = np.array(test_data, np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding image labels into target variables (categorical)\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features = [0])\n",
    "y_train = ohe.fit_transform(y_train.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19906, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network models - CNNs and its variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU, ELU\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras backend is set to Theano in keras.json file, hence external conversion to Tensorflow is required. In case, the keras backend is TensorFlow, by default, this will not be required.\n",
    "Use keras.backend.backend() or K.backend() to know the current backend.\n",
    "\n",
    "We use Keras with Theano backend and 'tf' image_dim_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'keras.backend' from '/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.pyc'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "reload(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 : CNN with BatchNorm + with/without Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the same as the ones used in the previous version of this project - Stacked CNN with BatchNormalization layers and Dropout. It can be used with either of the training methods which are shown below in the following sections.\n",
    "\n",
    "A little bit of architecture tuning was tried out by throwing in a couple more Dropout/BatchNorm/Dense/Conv-Pool layers and/or by changing the dropout probability, number of filters, optimizer and other such hyperparameters to arrive at the below model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(input_shape = (32,32,3)),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dropout(0.3),\n",
    "        Dense(384, activation='relu'),\n",
    "        Dropout(0.6),\n",
    "        Dense(3, activation='softmax')\n",
    "        ])\n",
    "model.compile(optimizer = 'adam' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving initial un-trained weights for future use\n",
    "model.save_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 128, 128, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 126, 126, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 124, 124, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 124, 124, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 60, 60, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 58, 58, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 58, 58, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 27, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 27, 27, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 961,967\n",
      "Trainable params: 961,321\n",
      "Non-trainable params: 646\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : Same as Model 1, but with image size resized to 128 and hence, more Conv-Pool layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few experiments tried were around hyperparameter tuning of these CNN networks.\n",
    "\n",
    "__Hyper-parameters__ : \n",
    "* *Batch Size ($32, 64, 128$ etc.)* : Batch sizes are selected keeping in mind the memory constraints of the hardware being used. Smaller batch sizes take a little longer to train, but are the only solution if our inputs are large. Different batch-sizes performed similarly, but a batch size of 32 turned out to be most suitable keeping all considerations in mind\n",
    "\n",
    "* *Image size ($128X128, 64X64, 32X32$)* : Most input images are large enough (>> 32X32). Hence, using an input image size of 128X128 seems the best option. Resizing the images to 32X32 causes significant blurring in most of the images. This might affect the accuracy of the trained model. However, a network with input images of size 128X128 takes ~25x longer to train than a similar 32X32 network. I tried using 128X128 images, albeit with more Conv-Pool layers, but the resultant accuracy wasn't significantly better. Hence, switched to 32X32. Even networks with 64X64 sized images didn't result in a significant gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(input_shape = (128,128,3)),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dropout(0.3),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.6),\n",
    "        Dense(3, activation='softmax')\n",
    "        ])\n",
    "model.compile(optimizer = 'adam' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving initial un-trained weights for future use\n",
    "model.save_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 : Stacked CNN + BatchNorm/Dropout with Advanced Activation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a ReLU activation generally works with a stacked CNN model (there's enough literature to prove its superiority over sigmoid and tanh), since the input images were significantly downsampled and hence, highly blurred, I tried using other activation layers as an alternative to ReLU to see if we can arrive at better weights.\n",
    "\n",
    "The other activations tried were :\n",
    "\n",
    "* Leaky ReLU (with different alpha values)\n",
    "* PReLU\n",
    "* ELU\n",
    "\n",
    "Leaky ReLU with a default alpha of 0.3 used in conjunction with Model 1's architecture helped reach ~75% test accuracy after 10-15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(input_shape = (32,32,3)),\n",
    "        Convolution2D(32,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(32,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        Convolution2D(64,(3,3), activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dropout(0.3),\n",
    "        Dense(384, activation='linear'),\n",
    "        LeakyReLU(alpha = 0.3),\n",
    "        Dropout(0.6),\n",
    "        Dense(3, activation='softmax')\n",
    "        ])\n",
    "model.compile(optimizer = 'adam' , loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving initial un-trained weights for future use\n",
    "model.save_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 384)               614784    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1155      \n",
      "=================================================================\n",
      "Total params: 682,287\n",
      "Trainable params: 681,897\n",
      "Non-trainable params: 390\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below section contains a function for learning rate decay / annealing along with 3 different ways of training.\n",
    "\n",
    "* Without data augmentation - Doesn't work well mostly. Overfits easily with most architectures\n",
    "* Data augmentation without cross-validation\n",
    "* Data augmentation with cross-validation\n",
    "\n",
    "Data augmentation helps in reducing overfitting and also helps in improving the accuracy for this particular problem, since most of the images are noisy (affected by blur, poor illumination, scaling effects etc.). I separated the cross-validation part so as to quickly try out different models with only one validation set. Once we finalize on a few architectures/set of hyperparameters, learning rate schedule, we can perform cross-validation to get a better estimation of validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Building a custom function for learning rate decay/annealing\n",
    "def lr_decay(start, stop, div_step_1, div_step_2 = 2) :\n",
    "    k = 1\n",
    "    while start >= stop:\n",
    "        yield start\n",
    "        if k==1 :\n",
    "            start/= div_step_1\n",
    "        else : start/= div_step_2\n",
    "        k = k * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Without data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data augmentation is not required, we can simply use the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving weights as a form of model checkpointing\n",
    "model.save_weights('age-detection_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data augmentation without cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : \n",
    "\n",
    "* When using networks with large images(e.g. 128 X 128), batch_size for train and validation set should be kept low, so as to fit the images in the RAM/vRAM\n",
    "\n",
    "* Each epoch with 128X128 images took ~4800-5200 secs, which makes it infeasible to train for a long time. Either use a faster GPU, or resize all images to 32X32 and train the model on blurred images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_trn, X_valid, y_trn, y_valid = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, shear_range=0.25,\n",
    "                               height_shift_range=0.1, zoom_range=0.2, horizontal_flip = True)\n",
    "batches = gen.flow(X_trn, y_trn, batch_size = 64)\n",
    "val_batches = gen.flow(X_valid, y_valid, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "248/248 [==============================] - 196s - loss: 0.6683 - acc: 0.7145 - val_loss: 0.6353 - val_acc: 0.7327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3153643fd0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, (X_trn.shape[0]//batches.batch_size), epochs=1,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]//val_batches.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "497/497 [==============================] - 223s - loss: 0.7815 - acc: 0.6540 - val_loss: 0.7287 - val_acc: 0.6843\n",
      "Epoch 2/2\n",
      "497/497 [==============================] - 222s - loss: 0.7593 - acc: 0.6679 - val_loss: 0.7406 - val_acc: 0.6628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f418a4190>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch = (X_trn.shape[0]//batches.batch_size), epochs=2,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]/val_batches.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LR decay can also be used for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in lr_decay(0.005, 0.0002, 2, 5):\n",
    "    model.optimizer.lr = i\n",
    "    print \"Learning rate = \" + str(i)\n",
    "    model.fit_generator(batches, (X_trn.shape[0]//batches.batch_size), epochs=1,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]//val_batches.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving weights as a form of model checkpointing\n",
    "model.save_weights('leakyReLU_age-detection_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data augmentation with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_idx, val_idx in skf.split(X_train,y_train):\n",
    "    print(\"Fitting fold %d\" %fold_num)\n",
    "    \n",
    "    # Data augmentation image generator\n",
    "    gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, shear_range=0.25,\n",
    "                               height_shift_range=0.1, zoom_range=0.2, horizontal_flip = True)\n",
    "    batches = gen.flow(X_train[train_idx], y_train[train_idx], batch_size = 32)\n",
    "    val_batches = gen.flow(X_train[val_idx], y_train[val_idx], batch_size = 64)\n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit_generator(batches, steps_per_epoch = (X_trn.shape[0]//batches.batch_size), epochs=3,\n",
    "                    validation_data = val_batches, validation_steps = (X_valid.shape[0]//val_batches.batch_size))\n",
    "    \n",
    "    # Recompiling the model with initial weights\n",
    "    model.load_weights('initial_weights.h5')\n",
    "    \n",
    "    fold_num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders with ConvNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Building the autoencoder pipeline\n",
    "\n",
    "input_img = Input(shape=(32, 32, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (8, 4, 4, 3) i.e. 384-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding = 'same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Composing the autoencoder model and compiling it\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy') # can change optimizer to 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 8)           1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 16)          1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 3)         867       \n",
      "=================================================================\n",
      "Total params: 13,939\n",
      "Trainable params: 13,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorboard to visualize the training process\n",
    "# Run this in the terminal\n",
    "!tensorboard --logdir = /tmp/autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19906 samples, validate on 6636 samples\n",
      "Epoch 1/10\n",
      "19906/19906 [==============================] - 122s - loss: 0.6284 - val_loss: 0.6159\n",
      "Epoch 2/10\n",
      "19906/19906 [==============================] - 120s - loss: 0.6025 - val_loss: 0.5934\n",
      "Epoch 3/10\n",
      "19906/19906 [==============================] - 122s - loss: 0.5946 - val_loss: 0.5942\n",
      "Epoch 4/10\n",
      "19906/19906 [==============================] - 122s - loss: 0.5913 - val_loss: 0.5889\n",
      "Epoch 5/10\n",
      "19906/19906 [==============================] - 124s - loss: 0.5891 - val_loss: 0.5887\n",
      "Epoch 6/10\n",
      "19906/19906 [==============================] - 123s - loss: 0.5873 - val_loss: 0.5862\n",
      "Epoch 7/10\n",
      "19906/19906 [==============================] - 125s - loss: 0.5860 - val_loss: 0.5875\n",
      "Epoch 8/10\n",
      "19906/19906 [==============================] - 125s - loss: 0.5852 - val_loss: 0.5847\n",
      "Epoch 9/10\n",
      "19906/19906 [==============================] - 124s - loss: 0.5844 - val_loss: 0.5851\n",
      "Epoch 10/10\n",
      "19906/19906 [==============================] - 123s - loss: 0.5839 - val_loss: 0.5834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1071911a90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting the autoencoder model\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs= 10,\n",
    "                batch_size= 64,\n",
    "                shuffle= True,\n",
    "                validation_data= (X_test, X_test))\n",
    "                #callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19906 samples, validate on 6636 samples\n",
      "Epoch 1/3\n",
      "19906/19906 [==============================] - 109s - loss: 0.5836 - val_loss: 0.5829\n",
      "Epoch 2/3\n",
      "19906/19906 [==============================] - 109s - loss: 0.5834 - val_loss: 0.5834\n",
      "Epoch 3/3\n",
      "19906/19906 [==============================] - 116s - loss: 0.5833 - val_loss: 0.5835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10c4061b10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs= 3,\n",
    "                batch_size= 128,\n",
    "                shuffle= True,\n",
    "                validation_data= (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6636, 32, 32, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting predicted category numbers to predicted labels\n",
    "unique_labels = np.unique(train['Class'].tolist())\n",
    "pred_labels = unique_labels[predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f317283c590>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN/9rkc7OudYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f317283c6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's see what our classifier predicts on test images\n",
    "# Random predictions\n",
    "i = np.random.choice(np.arange(len(test_data)))\n",
    "plt.title('{} - Predicted class : {}'.format(test['ID'].values[i], pred_labels[i]))\n",
    "plt.imshow(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare submission file\n",
    "subm = pd.DataFrame({'Class':pred_labels, 'ID':test.ID})\n",
    "subm.to_csv('sub03_2_leakyReLU.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
